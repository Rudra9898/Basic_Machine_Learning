{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccff352f",
   "metadata": {},
   "source": [
    "#### Use famous iris flower dataset from sklearn.datasets to predict flower species using random forest classifier.\n",
    "\n",
    "1. Measure prediction score using default n_estimators (10)\n",
    "2. Now fine tune your model by changing number of trees in your classifer and tell me what best score you can get using how many trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749f5e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b97ba337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6487a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4dc6ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR',\n",
       " 'data',\n",
       " 'data_module',\n",
       " 'feature_names',\n",
       " 'filename',\n",
       " 'frame',\n",
       " 'target',\n",
       " 'target_names']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6741d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "492d1b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d9f7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target']=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fea70af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3  target\n",
       "0  5.1  3.5  1.4  0.2       0\n",
       "1  4.9  3.0  1.4  0.2       0\n",
       "2  4.7  3.2  1.3  0.2       0\n",
       "3  4.6  3.1  1.5  0.2       0\n",
       "4  5.0  3.6  1.4  0.2       0\n",
       "5  5.4  3.9  1.7  0.4       0\n",
       "6  4.6  3.4  1.4  0.3       0\n",
       "7  5.0  3.4  1.5  0.2       0\n",
       "8  4.4  2.9  1.4  0.2       0\n",
       "9  4.9  3.1  1.5  0.1       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9741417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28b2b52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[['target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "961b2a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c18c34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65ab5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(df.drop('target',axis='columns'),df.target,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0380c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rcf=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45823b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0faf878d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98dec3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=rcf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32997552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b6129ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm,annot=True)\n",
    "plt.xlabel(\"y_test\")\n",
    "plt.ylabel(\"y_pred\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "867ce60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RandomForestClassifier in module sklearn.ensemble._forest object:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `round(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      " |      Only available if bootstrap=True.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeClassifier\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      " |          removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      " |  sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
      " |      tree classifiers.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(...)\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest.\n",
      " |      The class probability of a single tree is the fraction of samples of\n",
      " |      the same class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |      \n",
      " |      Number of features when fitting the estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4470e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc1=RandomForestClassifier(n_estimators=10)\n",
    "rfc1.fit(X_train,y_train)\n",
    "rfc1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63b710ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc2=RandomForestClassifier(n_estimators=20)\n",
    "rfc2.fit(X_train,y_train)\n",
    "rfc2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8795401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
